interface AIAnalysisResult {
  recommendation: 'approve' | 'reject' | 'escalate';
  confidence: number;
  riskScore: number;
  detectedContent: string[];
  contentTags: string[];
  flaggedReasons: string[];
  ensembleResults: {
    nsfwDetection: { score: number; labels: string[] };
    csamDetection: { score: number; riskLevel: 'none' | 'low' | 'medium' | 'high' | 'critical' };
    hateSpeechDetection: { score: number; categories: string[] };
    weaponsDetection: { score: number; detected: string[] };
    violenceDetection: { score: number; level: string };
  };
  humanReviewRequired: boolean;
  escalationReason?: string;
}

export class AIModerationService {
  
  async analyzeContent(mediaAsset: any): Promise<AIAnalysisResult> {
    // Enhanced ensemble AI analysis with multiple detection models
    console.log(`üîç Starting ensemble AI analysis for media: ${mediaAsset.id || 'unknown'}`);
    
    const analysis: AIAnalysisResult = {
      recommendation: 'approve',
      confidence: 0,
      riskScore: 0,
      detectedContent: [],
      contentTags: [],
      flaggedReasons: [],
      ensembleResults: {
        nsfwDetection: { score: 0, labels: [] },
        csamDetection: { score: 0, riskLevel: 'none' },
        hateSpeechDetection: { score: 0, categories: [] },
        weaponsDetection: { score: 0, detected: [] },
        violenceDetection: { score: 0, level: 'none' }
      },
      humanReviewRequired: false
    };

    try {
      // Run ensemble detection models in parallel
      const [nsfwResult, csamResult, hateSpeechResult, weaponsResult, violenceResult] = await Promise.all([
        this.detectNSFWContent(mediaAsset),
        this.detectCSAMContent(mediaAsset),
        this.detectHateSpeech(mediaAsset),
        this.detectWeapons(mediaAsset),
        this.detectViolence(mediaAsset)
      ]);

      analysis.ensembleResults.nsfwDetection = nsfwResult;
      analysis.ensembleResults.csamDetection = csamResult;
      analysis.ensembleResults.hateSpeechDetection = hateSpeechResult;
      analysis.ensembleResults.weaponsDetection = weaponsResult;
      analysis.ensembleResults.violenceDetection = violenceResult;

      // Calculate composite risk score and recommendation
      await this.calculateCompositeScore(analysis);
      
      console.log(`üìä AI analysis complete: ${analysis.recommendation} (confidence: ${analysis.confidence}%, risk: ${analysis.riskScore})`);
      return analysis;
      
    } catch (error) {
      console.error('‚ùå AI analysis failed:', error);
      // Default to human review on AI failure
      analysis.recommendation = 'escalate';
      analysis.humanReviewRequired = true;
      analysis.escalationReason = 'AI analysis system failure';
      analysis.flaggedReasons.push('System error - requires human review');
      return analysis;
    }
  }

  // CRITICAL: NSFW Content Detection
  private async detectNSFWContent(mediaAsset: any): Promise<{ score: number; labels: string[] }> {
    try {
      // In production: Integrate with NSFW detection APIs like Sightengine, AWS Rekognition, or custom models
      console.log(`üîç NSFW detection for ${mediaAsset.mimeType}`);
      
      const nsfwLabels: string[] = [];
      let nsfwScore = 0;

      // Enhanced detection for images and videos
      if (mediaAsset.mimeType?.startsWith('image/') || mediaAsset.mimeType?.startsWith('video/')) {
        // Simulate production NSFW detection API call
        const mockDetection = this.simulateNSFWDetection(mediaAsset);
        nsfwScore = mockDetection.score;
        nsfwLabels.push(...mockDetection.labels);
        
        // In production, would call actual service:
        // const response = await fetch('https://api.sightengine.com/1.0/check.json', { ... });
        // const result = await response.json();
        // nsfwScore = result.nudity.sexual_activity + result.nudity.sexual_display;
      }

      return { score: nsfwScore, labels: nsfwLabels };
    } catch (error) {
      console.error('‚ùå NSFW detection failed:', error);
      return { score: 0, labels: ['detection_error'] };
    }
  }

  // CRITICAL: CSAM Detection (Child Safety)
  private async detectCSAMContent(mediaAsset: any): Promise<{ score: number; riskLevel: 'none' | 'low' | 'medium' | 'high' | 'critical' }> {
    try {
      console.log(`üõ°Ô∏è CSAM detection for ${mediaAsset.mimeType}`);
      
      // CRITICAL: Zero-tolerance for CSAM
      // In production: Use specialized CSAM detection services like PhotoDNA, Sightengine CSAM
      let csamScore = 0;
      let riskLevel: 'none' | 'low' | 'medium' | 'high' | 'critical' = 'none';

      if (mediaAsset.mimeType?.startsWith('image/') || mediaAsset.mimeType?.startsWith('video/')) {
        const mockDetection = this.simulateCSAMDetection(mediaAsset);
        csamScore = mockDetection.score;
        
        // CRITICAL: Immediate escalation for any CSAM detection
        if (csamScore > 0.1) {
          riskLevel = 'critical';
          console.log(`üö® CRITICAL: Potential CSAM detected - immediate escalation required`);
        }
      }

      return { score: csamScore, riskLevel };
    } catch (error) {
      console.error('‚ùå CSAM detection failed:', error);
      // Default to critical on error for safety
      return { score: 1.0, riskLevel: 'critical' };
    }
  }

  // Hate Speech Detection
  private async detectHateSpeech(mediaAsset: any): Promise<{ score: number; categories: string[] }> {
    try {
      console.log(`üéØ Hate speech detection for ${mediaAsset.mimeType}`);
      
      const categories: string[] = [];
      let score = 0;

      // Check text content (titles, descriptions, captions)
      const textContent = [mediaAsset.title, mediaAsset.description].filter(Boolean).join(' ').toLowerCase();
      
      if (textContent) {
        const mockDetection = this.simulateHateSpeechDetection(textContent);
        score = mockDetection.score;
        categories.push(...mockDetection.categories);
      }

      return { score, categories };
    } catch (error) {
      console.error('‚ùå Hate speech detection failed:', error);
      return { score: 0, categories: ['detection_error'] };
    }
  }

  // Weapons Detection
  private async detectWeapons(mediaAsset: any): Promise<{ score: number; detected: string[] }> {
    try {
      console.log(`‚öîÔ∏è Weapons detection for ${mediaAsset.mimeType}`);
      
      const detected: string[] = [];
      let score = 0;

      if (mediaAsset.mimeType?.startsWith('image/') || mediaAsset.mimeType?.startsWith('video/')) {
        const mockDetection = this.simulateWeaponsDetection(mediaAsset);
        score = mockDetection.score;
        detected.push(...mockDetection.detected);
      }

      return { score, detected };
    } catch (error) {
      console.error('‚ùå Weapons detection failed:', error);
      return { score: 0, detected: ['detection_error'] };
    }
  }

  // Violence Detection
  private async detectViolence(mediaAsset: any): Promise<{ score: number; level: string }> {
    try {
      console.log(`üí• Violence detection for ${mediaAsset.mimeType}`);
      
      let score = 0;
      let level = 'none';

      if (mediaAsset.mimeType?.startsWith('image/') || mediaAsset.mimeType?.startsWith('video/')) {
        const mockDetection = this.simulateViolenceDetection(mediaAsset);
        score = mockDetection.score;
        level = mockDetection.level;
      }

      return { score, level };
    } catch (error) {
      console.error('‚ùå Violence detection failed:', error);
      return { score: 0, level: 'error' };
    }
  }

  // Calculate composite risk score from ensemble results
  private async calculateCompositeScore(analysis: AIAnalysisResult): Promise<void> {
    const weights = {
      csam: 1000,    // CRITICAL: CSAM gets maximum weight
      nsfw: 30,      // High weight for NSFW content
      hateSpeech: 25,
      weapons: 20,
      violence: 15
    };

    // Calculate weighted risk score
    let riskScore = 0;
    riskScore += analysis.ensembleResults.csamDetection.score * weights.csam;
    riskScore += analysis.ensembleResults.nsfwDetection.score * weights.nsfw;
    riskScore += analysis.ensembleResults.hateSpeechDetection.score * weights.hateSpeech;
    riskScore += analysis.ensembleResults.weaponsDetection.score * weights.weapons;
    riskScore += analysis.ensembleResults.violenceDetection.score * weights.violence;

    analysis.riskScore = Math.min(riskScore, 100); // Cap at 100

    // CRITICAL: Immediate rejection for CSAM
    if (analysis.ensembleResults.csamDetection.riskLevel === 'critical') {
      analysis.recommendation = 'reject';
      analysis.confidence = 99;
      analysis.humanReviewRequired = true;
      analysis.escalationReason = 'CRITICAL: Potential CSAM detected';
      analysis.flaggedReasons.push('CSAM DETECTION - IMMEDIATE ESCALATION');
      return;
    }

    // High-risk content requires human review
    if (analysis.riskScore > 50) {
      analysis.recommendation = 'escalate';
      analysis.humanReviewRequired = true;
      analysis.escalationReason = 'High-risk content detected by ensemble AI';
    } else if (analysis.riskScore > 20) {
      analysis.recommendation = 'escalate';
      analysis.humanReviewRequired = true;
    } else {
      analysis.recommendation = 'approve';
    }

    // Calculate confidence based on consensus
    const detectionResults = [
      analysis.ensembleResults.nsfwDetection.score,
      analysis.ensembleResults.hateSpeechDetection.score,
      analysis.ensembleResults.weaponsDetection.score,
      analysis.ensembleResults.violenceDetection.score
    ];
    
    const consensus = detectionResults.filter(score => score > 0.5).length;
    analysis.confidence = Math.max(70, 95 - (consensus * 10));
  }

  // Mock detection methods (replace with real AI service integrations in production)
  private simulateNSFWDetection(mediaAsset: any): { score: number; labels: string[] } {
    const fileName = mediaAsset.title?.toLowerCase() || '';
    const nsfwPatterns = ['nude', 'naked', 'sex', 'adult', 'explicit', 'porn'];
    
    let score = Math.random() * 0.3; // Base random score
    const labels: string[] = [];

    for (const pattern of nsfwPatterns) {
      if (fileName.includes(pattern)) {
        score += 0.6;
        labels.push(`nsfw_${pattern}`);
      }
    }

    return { score: Math.min(score, 1.0), labels };
  }

  private simulateCSAMDetection(mediaAsset: any): { score: number } {
    const fileName = mediaAsset.title?.toLowerCase() || '';
    const csamPatterns = ['child', 'minor', 'young', 'underage', 'teen'];
    
    let score = 0;
    
    // CRITICAL: Any indication of underage content gets flagged
    for (const pattern of csamPatterns) {
      if (fileName.includes(pattern)) {
        score = 0.8; // High score for potential CSAM
        console.log(`üö® CSAM WARNING: Pattern "${pattern}" detected in filename`);
        break;
      }
    }

    return { score };
  }

  private simulateHateSpeechDetection(textContent: string): { score: number; categories: string[] } {
    const hateSpeechPatterns = {
      racial: ['racist', 'racial slur'],
      religious: ['religious hate'],
      sexist: ['sexist', 'misogyny'],
      homophobic: ['homophobic', 'lgbtq hate']
    };

    let score = 0;
    const categories: string[] = [];

    for (const [category, patterns] of Object.entries(hateSpeechPatterns)) {
      for (const pattern of patterns) {
        if (textContent.includes(pattern)) {
          score += 0.4;
          categories.push(category);
        }
      }
    }

    return { score: Math.min(score, 1.0), categories };
  }

  private simulateWeaponsDetection(mediaAsset: any): { score: number; detected: string[] } {
    const fileName = mediaAsset.title?.toLowerCase() || '';
    const weaponPatterns = ['gun', 'weapon', 'knife', 'firearm', 'rifle', 'pistol'];
    
    let score = 0;
    const detected: string[] = [];

    for (const weapon of weaponPatterns) {
      if (fileName.includes(weapon)) {
        score += 0.5;
        detected.push(weapon);
      }
    }

    return { score: Math.min(score, 1.0), detected };
  }

  private simulateViolenceDetection(mediaAsset: any): { score: number; level: string } {
    const fileName = mediaAsset.title?.toLowerCase() || '';
    const violencePatterns = ['violence', 'fight', 'attack', 'assault', 'murder'];
    
    let score = 0;
    let level = 'none';

    for (const pattern of violencePatterns) {
      if (fileName.includes(pattern)) {
        score += 0.4;
        level = score > 0.7 ? 'high' : score > 0.4 ? 'medium' : 'low';
      }
    }

    return { score: Math.min(score, 1.0), level: level };
  }

  async processAutoModeration(mediaAsset: any): Promise<{
    action: 'auto_approve' | 'auto_reject' | 'human_review';
    analysis: AIAnalysisResult;
  }> {
    const analysis = await this.analyzeContent(mediaAsset);
    
    // Auto-approve low risk content with high confidence
    if (analysis.recommendation === 'approve' && analysis.confidence > 90 && analysis.riskScore < 10) {
      return { action: 'auto_approve', analysis };
    }
    
    // Auto-reject very high risk content with high confidence
    if (analysis.recommendation === 'reject' && analysis.confidence > 90 && analysis.riskScore > 80) {
      return { action: 'auto_reject', analysis };
    }
    
    // Everything else goes to human review
    return { action: 'human_review', analysis };
  }

  async flagContent(mediaId: string, reason: string, severity: 'low' | 'medium' | 'high' | 'critical'): Promise<void> {
    // This would integrate with the kill switch system
    console.log(`Content ${mediaId} flagged: ${reason} (severity: ${severity})`);
    
    // Critical flags trigger immediate takedown
    if (severity === 'critical') {
      // Trigger kill switch protocol
      await this.triggerKillSwitch(mediaId, reason);
    }
  }

  private async triggerKillSwitch(mediaId: string, reason: string): Promise<void> {
    // Immediate content takedown protocol
    console.log(`KILL SWITCH ACTIVATED for ${mediaId}: ${reason}`);
    // Implementation would:
    // 1. Immediately hide content from public view
    // 2. Notify all administrators
    // 3. Create high-priority audit log
    // 4. Send alerts to compliance team
  }

  async generateComplianceReport(mediaId: string): Promise<any> {
    return {
      mediaId,
      complianceStatus: 'reviewed',
      aiAnalysisDate: new Date(),
      humanReviewDate: new Date(),
      finalDecision: 'approved',
      reviewerNotes: 'Content meets platform guidelines',
      auditTrail: []
    };
  }
}

export const aiModerationService = new AIModerationService();